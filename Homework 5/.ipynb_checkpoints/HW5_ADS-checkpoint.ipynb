{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The notebook should be named as ADS_HW5_<net_id>\n",
    "# 10 pts will be deducted if the naming convention is not followed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 (5 pts) \n",
    "\n",
    "\n",
    "a) Provide a definition for the concept of a conjugate prior given the Bayesian learning model\n",
    "\n",
    "b) True or False:\n",
    "Selecting a Gaussian prior for a likelihood function that is Gaussian, will ensure that the posterior distribution is also Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (5 pts). \n",
    "\n",
    "\n",
    "1) In the Bayesian linear regression framework (True or False)  \n",
    "    a) Using a Laplacian prior will lead to the Ridge Regression  \n",
    "    b) Using a Laplacian prior will lead to Lasso Regression\n",
    "\n",
    "2) (True or False)  \n",
    "    a) Ridge can often help with feature selection  \n",
    "    b) Lasso can often help with feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 (10 pts). \n",
    "Answer some questions about the properties of Lasso/Ridge:\n",
    "\n",
    "1.a) Which regression adds penalty equivalent to square of the magnitude of coefficients?  \n",
    "1.b) Which regression adds penalty  equivalent to absolute value of the magnitude of coefficients?\n",
    "\n",
    "2) Which of the Lasso and Ridge regressions possess an analytic solution in the closed form?\n",
    "\n",
    "3) Which of the Lasso and Ridge regression is often use to completely eliminate coefficients for some of the regressors?\n",
    "\n",
    "4) Which value of $\\lambda$ makes Lasso and Ridge identical to the ordinary least square regression?\n",
    "\n",
    "5) What dataset (training, validation or test) you would use to estimate the $\\lambda$ for Lasso or Ridge?\n",
    "\n",
    "# Extra Credit(50 pts to be applied towards this or any other homework)\n",
    "For a regression where the variables are highly correlated, which would you use, Lasso or Ridge and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (15 pts). \n",
    "Consider a Bayesian univariate linear regression $y\\sim {\\cal N}(wx,\\sigma^2)$ with a given $\\sigma=2$ and the prior $w\\sim {\\cal N}(4,2)$. Compute the posterior distribution after an observation $y=14,x=4$. After all, what is the probability of having $w<4$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 (20 pts)\n",
    "For the training and test sets provided below, train a linear regression of $y$ (last column) vs the rest of the columns of the table treated as regressors (intercept excluded) over the training set, apply it to the test set and report it's R2 performance over both - training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Learn to use numpy - Will be more useful in future\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_train=pd.read_csv(\"HW_train.csv\")\n",
    "data_test=pd.read_csv(\"HW_test.csv\")\n",
    "\n",
    "# Remove unwanted column\n",
    "data_train.drop('Unnamed: 0',axis=1, inplace=True)\n",
    "data_test.drop('Unnamed: 0',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6 (20 pts)\n",
    "Train Lasso and Ridge regressions using the training sample above with $\\lambda_{Ridge}=3000$ and $\\lambda_{Lasso}=30$ and report their performance over the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 (25 pts)\n",
    "a) For each $m=0,1,2,...39$ train OLS, Lasso and Ridge regressions(keep intercept) using the data from Task 5 above with $\\lambda_{Ridge}=3000$ and $\\lambda_{Lasso}=30$ using the first $m$ columns of the data tables as the regressors  \n",
    "i.e train the models by adding in a new variable each time. eg. Y~ x0, Y~x0+x1, Y~x0+x1+x2...Y~x0+x1+...+x39  \n",
    "\n",
    "b) Plot the in-sample (training) and out-of-sample (test) R2 for all three models on the same graph.  \n",
    "i.e (x-axis is number of regressors, y will be your R^2 for InSample & OutSample for each of the 3 models, all in one graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracredit (100 pts of credit to be applied towards this or any other homework)\n",
    "Using a series of 10 random splits (cross-validation) of the training sample into approximately 70% training and 30% validation samples perform the selection of the optimal $\\lambda$ for Ridge and Lasso regression:   \n",
    "\n",
    "a) For each $\\lambda$ of an assumed sequence of $\\lambda$'s(take a random range) compute validation R2 for all the 10 splits and finally select the $\\lambda$ having the highest value for validation R2 for each split.  \n",
    "\n",
    "b) Visualize the dependence of the validation R2 over $\\lambda$s for Lasso and Ridge(x-axis = $\\lambda$s from sequence, y-axis = corresponding R^2, graph for each split)\n",
    "\n",
    "c) Report the R2 computed over the test set for the Lasso and Ridge trained over the entire training set with the selected optimal values of $\\lambda$ out of the 10 splits."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
